
# 大语言模型（LLM）面试知识手册

涵盖预训练、SFT、DPO、PPO、推理优化、注意力机制等核心问答

# 前言：

本手册整理了当前主流大语言模型训练与部署过程中的关键技术点，涵盖：

	•	模型训练流程全景： 从预训练、SFT、RM、PPO 到 DPO / SimPO / ORPO 的完整路径
	•	推理优化机制： 包括 vLLM、llama.cpp、FlashAttention、KV Cache 等底层加速技术
	•	注意力机制深入拆解： Self-Attention、GQA、MQA、RoPE、位置编码等重点机制
	•	面试高频问题总结： 以“问答”形式汇总原理、优劣、对比、应用场景
	•	适配部署实践： 涵盖 V100 显卡上不同引擎（vLLM、llama.cpp）的实际性能参考

本手册适合：

	•	面试 LLM/NLP 相关岗位的准备阶段
	•	LLM 实践者、工程师梳理知识框架
	•	构建或调优大模型系统时的技术参考

使用建议

	•	对照岗位要求 复习对应模块（如偏好优化或推理引擎优化）
	•	快速查阅关键术语（如 FlashAttention 与传统 attention 区别、DPO 训练流程）
	•	在面试中灵活应用，结合项目经验、加深理解效果最佳


##  1. 预训练是做什么的？
	•	目标：从海量无标注文本中学语言能力（通常一次性训练大模型）
	•	特点：训练耗时、成本高，但是基础中的基础
	•	是否自己做：一般使用开源预训练模型，无需重复训练

##  2. 指令监督微调（SFT）是做什么的？
	•	目标：让模型学会执行人类指令（如问答、对话）
	•	方式：使用高质量“指令-回答”对做有监督训练
	•	适用：适合构建基础可控模型，例如 chat 模式

##  3. 奖励模型训练（RM）是做什么的？
	•	目标：训练一个模型来评分输出“好不好”（打分器）
	•	用途：给后续的强化学习训练（PPO 等）提供 reward
	•	难点：数据构建难，通常需要人类偏好对比（两个答案哪个更好）

##  4. PPO（Proximal Policy Optimization）是做什么的？
	•	目标：基于 reward（RM 输出）优化模型策略
	•	特点：传统 RLHF 强化学习方法，收敛慢，难调参
	•	效果：输出多样化、有创造力，但训练代价大

##  5. DPO（Direct Preference Optimization）是做什么的？
	•	目标：跳过奖励模型，直接用偏好对比做优化
	•	优点：比 PPO 简单，不需要 reward model
	•	效果：训练稳定性好，适合偏好对比数据（成对答案）
	
##  6. KTO / ORPO / SimPO的特点和简介？
| 方法  | 特点         | 简介                                                              |
|-------|--------------|-------------------------------------------------------------------|
| KTO   | 数据顺序敏感 | Keep The Order：保留偏好顺序信息                                  |
| ORPO  | 在线排序优化 | Online Rank Preference Optimization，处理多个选项的排序关系       |
| SimPO | 利用相似性评分 | 使用 Cosine 等方法衡量输出间偏好差异，训练稳定、效果好               |

##  7. LoRA / QLoRA的简介？
	•	是一种训练方式/技巧，而不是训练目标
	•	用来替代“全参数训练”，只训练少量插入层，显存省、训练快
	•	QLoRA 是量化版本（通常 4bit），更节省资源

## 8.应该如何选择这些技术方法？
| 目标                 | 推荐方式             |
|----------------------|----------------------|
| 基础问答、对话         | 指令微调（SFT） + LoRA   |
| 控制输出质量、有偏好排序  | DPO / ORPO / SimPO      |
| 追求效果极致（预算高）     | PPO（需 RM）          |
| 资源紧张             | QLoRA + SimPO         |
| 不想构建 RM          | DPO / SimPO 更简单      |

## 9.如何从零开始训练一个大模型，文案解析。

```c
预训练（Pretraining）
                   ↓
     指令微调 SFT（Supervised Fine-tuning）
                   ↓
   偏好数据准备（人类偏好标注或对比）
                   ↓
 ┌─────────────┬─────────────┐
 ↓                           ↓
奖励模型 RM              直接偏好优化（DPO / SimPO）
   ↓                               ↓
PPO 强化训练             或完成优化训练（完成）
```
###  9.1.简要说明流程逻辑：
| 阶段                | 必选性            | 说明                              |
|---------------------|-------------------|-----------------------------------|
| 预训练              | ✅ 必要（已完成）   | 模型基础语言能力来自这里              |
| SFT（指令微调）     | ✅ 强烈建议        | 让模型能听懂指令，做任务              |
| RM（奖励模型）      | ❌ 可选           | 给 PPO 提供 reward，用于偏好强化      |
| PPO                 | ❌ 可选           | 强化训练，调优输出风格，但训练难度高    |
| DPO / SimPO / ORPO  | ✅ 建议（替代 PPO） | 不依赖 reward，更稳定、更轻量          |
###  9.2.总结一句话：
SFT 是微调的“第一步”，DPO / SimPO 是第二步的“进阶优化”，可以只做 SFT 也能上线，但想要更强的模型偏好控制、效果提升，就得进一步用 DPO 或 PPO。



## 10.PPO的原理、公式；DPO的原理、公式、数据；KTO原理；ORPO原理；
[《第一篇》大模型强化学习中的 PPO、DPO、KTO、ORPO 全解析](https://blog.csdn.net/guoguozgw/article/details/148312406)



## 11.简述DPO的训练流程？
[《第二篇》DPO（Direct Preference Optimization）详解](https://blog.csdn.net/guoguozgw/article/details/148326652)


## 12.什么是思维链？
CoT（Chain of Thought）含义

在传统的提示或训练中，我们通常直接输入问题并期望模型给出答案：

输入： 7 个苹果加 5 个苹果是多少？
输出： 12

而在 思维链提示 中，我们希望模型模拟人类推理过程，先展示中间步骤，再得出答案：

输入： 7 个苹果加 5 个苹果是多少？
输出： 首先有 7 个苹果，再加上 5 个苹果，7 + 5 = 12，所以答案是 12。



为什么使用 CoT？
	•	增强模型推理能力：特别是在数学题、多步推理、复杂逻辑等任务上；
	•	提升可解释性：模型的每一步推理都可追踪；
	•	减少幻觉：强制模型考虑中间过程，降低“拍脑袋瞎说”的情况。


CoT 在 Med-R1 中的作用

在医学领域问题中，例如问诊推理或病因分析，一步得答案往往不可靠，CoT 可以帮助模型：
	•	按步骤梳理症状 → 假设疾病 → 推导诊断；
	•	把复杂指令分解为小任务逐步解决；
	•	模拟医生“思考”过程，更贴近人类专家逻辑。


总结一句话

CoT（思维链）就是让模型像人一样“分步骤思考”，不是直接跳出答案，而是把中间推理过程也展示出来。

## 13.讲解一下你所知道的注意力机制，在大模型中的。
[Self-Attention（自注意力）Cross-Attention（跨注意力）Multi-Head Attention（多头注意力）](https://blog.csdn.net/guoguozgw/article/details/148333759?sharetype=blogdetail&sharerId=148333759&sharerefer=PC&sharesource=guoguozgw&spm=1011.2480.3001.8118)


## 14.KV Cache的核心机制是什么？

[《第四篇》KV Cache：大语言模型推理加速的核心机制详解](https://blog.csdn.net/guoguozgw/article/details/148394992)
## 15.谈一下VLLM的简介；


[《第五篇》vLLM：让大语言模型推理更高效的新一代引擎 —— 原理详解](https://blog.csdn.net/guoguozgw/article/details/148395591)
### Q1：vLLM 是什么？它的主要用途是什么？

**答：**
vLLM 是一个基于 Transformer 的高效大语言模型推理引擎，主要用于加速 LLaMA、ChatGLM、Phi-3 等开源模型的推理过程，特别适合本地部署和高并发服务。



### Q2：vLLM 中的 PagedAttention 是什么？解决了什么问题？

**答：**
PagedAttention 是 vLLM 的核心创新之一，它借鉴操作系统的分页机制，将每个 token 的 K/V 缓存分成“块”，按需分配和释放，解决了传统 KV Cache 显存浪费和长文本推理困难的问题。


### Q3：KV Cache 是什么？为什么它对推理效率很重要？

**答：**
KV Cache 是用于缓存 attention 中的 Key 和 Value 向量的结构。它避免了重复计算历史 token 的 K/V，从而大幅提升了自回归生成的效率。


### Q4：vLLM 支持哪些注意力机制优化？

**答：**
vLLM 支持 FlashAttention-2、Grouped Query Attention (GQA)、Multi-Query Attention (MQA)，这些都能有效降低显存占用并提升推理速度。



### Q5：vLLM 如何进行多 GPU 并行推理？

**答：**
vLLM 支持 Tensor Parallelism，只需设置 `tensor_parallel_size=N`，即可自动将模型分布到多个 GPU 上进行推理。


### Q6：vLLM 是否支持流式输出？如何实现？

**答：**
是的，vLLM 支持流式输出（Streaming），可以通过 `use_tqdm=False` + `stream=True` 参数实现逐词生成。


### Q7：vLLM 与 HuggingFace Transformers 相比有哪些优势？

**答：**
- 显存占用更低
- 吞吐量更高（2~3x）
- 支持长上下文和大批量推理
- 自动优化 KV Cache 和 attention 计算

---

### Q8：vLLM 是否支持量化？如何启用？

**答：**
vLLM 支持 INT8 量化推理，可通过 `quantization="AWQ"` 或 `"INT8"` 参数启用，显著降低显存需求。

## 16.llama.cpp的见解；

[《第六篇》llama.cpp：纯 C/C++ 实现的大语言模型推理引擎详解](https://blog.csdn.net/guoguozgw/article/details/148396630)
## 17.俩个对比的真是效果是什么？

以下是在 V100 32GB 上使用 vLLM 部署 Qwen-14B 的典型性能估算：

模型加载：

    --dtype half (FP16): 显存占用 ~28 GB
    --quantization awq (4-bit): 显存占用 ~10-12 GB (可显著提升吞吐和并发)

吞吐量 (Throughput)：

    无量化 (FP16): ~150 - 250 tokens/s
    AWQ 量化 (4-bit): ~250 - 400+ tokens/s

并发能力 (Concurrency)：

    假设平均每个请求生成 512 个 token。
    在 FP16 模式下，理论上可以支持 10-20 个并发。
    在 AWQ 4-bit 模式下，由于显存和计算效率提升，可以轻松支持 30-50 个并发。

注意：实际性能会受到 max_model_len、max_num_batched_tokens、请求的平均长度和波动、CPU 性能（用于分词/合词）等多种因素影响。 
## 18.Flashattention的机制

[《第七篇》FlashAttention：高效注意力计算的核心机制详解](https://blog.csdn.net/guoguozgw/article/details/148420360)

## 19.Flashattention为什么可以降低缓存？

[《第八篇》FlashAttention 详解：为什么它能显著降低显存缓存？《一》](https://blog.csdn.net/guoguozgw/article/details/148420604)

## 20. Flashattention为什么可以扩大支持的 token 数量？

### 显存是长文本的最大瓶颈

- 对于 32k tokens，标准 attention 的 $$ QK^T $$ 矩阵占用了大量显存（可达 2GB 以上）
- 大多数消费级 GPU（如 RTX 3090）无法支持

### FlashAttention 的解决方案

- 分块处理，避免一次性加载全部 attention 矩阵
- 流式计算与释放，适配长文本建模
- 支持高达 32k tokens 的上下文长度

实验表明，FlashAttention-2 可在 RTX 3090 上训练 32k tokens 的上下文，而传统方法早已爆显存。



## 21. Flashattention为什么并行化程度更高？

### 并行瓶颈分析

传统 attention 存在线程同步和显存读写的强依赖，导致并行效率受限。

### FlashAttention 的并行优化

- **Tile-based 并行策略**：每个 tile 可独立计算，互不干扰
- **Shared Memory + Register 优化**：提升数据访问速度
- **指令级并行**：充分利用 GPU 的 warp-level 执行能力

 性能测试显示：
- FA-2 比 FA-1 吞吐量提升 1.5~2x
- 并行性更强，适合大规模分布式训练



## 22. FlashAttention-1 与 FlashAttention-2 的区别

- **FA-1**：仅适用于 full self-attention，tile 划分不够灵活
- **FA-2**：支持任意 attention pattern，tile 划分更细粒度，更适合现代 Transformer 架构（如 Grouped Query Attention、Sliding Window、Sparse Attention）


## 23.MQA是什么？

[《第九篇》MQA 详解：多查询注意力机制的原理与应用](https://blog.csdn.net/guoguozgw/article/details/148422448)

## 24.RoPE 旋转位置编码的原理与实现方式是什么？

[《第十篇》RoPE 详解：旋转位置编码的原理与实践](https://blog.csdn.net/guoguozgw/article/details/148424088)

## 25.Grouped Query Attention（GQA）的基本原理与实现步骤是怎样的？

[《第十一篇》GQA（Grouped Query Attention）：分组注意力机制的原理与实践《一》](https://blog.csdn.net/guoguozgw/article/details/148430017)


## 27.PPO 算法的原理、公式和代码实现？

[《第二十三篇》一文搞懂PPO算法：原理、公式、代码实现全覆盖](https://blog.csdn.net/guoguozgw/article/details/148445360)

## 28.DPO 的原理、公式和应用场景？

[《第二十四篇》搞懂 DPO 只需一篇文章：原理、公式与应用场景解析](https://blog.csdn.net/guoguozgw/article/details/148456293)

## 29.Advantage 和 GAE 的核心原理是什么？

[策略梯度核心：Advantage 与 GAE 原理详解](https://blog.csdn.net/guoguozgw/article/details/148456286)


## 30.BPE、WordPiece、Unigram是干什么的？

[BPE、WordPiece 与 Unigram：三种主流子词分词算法对比](https://blog.csdn.net/guoguozgw/article/details/148311714)
